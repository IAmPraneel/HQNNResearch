# Hybrid Quantum Neural Network (HQNN)

A hybrid classicalâ€“quantum neural network (HQNN) designed to explore quantum advantages in machine learning for regression tasks on structured datasets.

> ğŸš§ This project is a work in progress. Contributions, feedback, and collaboration inquiries are welcome.

---

## âœ¨ Overview

This project combines classical neural networks with quantum circuits using Qiskit and PyTorch to build a hybrid model for predictive tasks. The goal is to evaluate whether quantum-enhanced layers can contribute to learning efficiency or expressivity in practical settings.

---
## ğŸ“Œ Latest update: Switched to pennylane, implemented 27 HQNN models on subset of dataset(1K samples) for 20 epochs. Recording the logs, parameters and metrics. (9 models, each 3 times with different seed for rigorous results). Based on summary performance 5 models were selected as 'best' and out of those based on epoch wise analysis 2 were chosen for noisy simulation/real quantum hardware tests (IBM Runtime). 

---

## ğŸ§  Approach

- **Pre-Autoencoder**: Reduces input dimensionality using classical neural layers.
- **Quantum Circuit**: Parametrized unitary operations and entanglement for core processing.
- **Post-Autoencoder**: Classical decoding layer for final regression output.
- **Full Pipeline**: Fully differentiable, GPU-accelerated, and designed for mixed precision.

---

## ğŸ›  Tech Stack

| **Component**             | **Technology / Library**                                |
|---------------------------|----------------------------------------------------------|
| **Deep Learning**         | PyTorch (`TorchConnector` from Qiskit ML)               |
| **Quantum Integration**   | Qiskit, Qiskit Machine Learning, Qiskit Aer, cuQuantum, pennylane   |
| **Quantum Backend**       | AerSimulator with GPU support (`cuStateVec`)            |
| **Automatic Differentiation** | PyTorch Autograd with Qiskit EstimatorQNN        |
| **Mixed Precision**       | `torch.cuda.amp` (Autocast & GradScaler)                |
| **Optimization**          | AdamW, ReduceLROnPlateau                                 |
| **Data Handling**         | pandas, NumPy, StandardScaler                            |
| **Progress Tracking**     | tqdm                                                     |
| **GPU Acceleration**      | NVIDIA CUDA (`torch.device("cuda")`)                    |
| **Model Checkpointing**   | `torch.save`, `torch.load`                              |
| **Evaluation & Splits**   | scikit-learn (`train_test_split`)                       |


---

## ğŸ”§ Current Status

- âœ… Model definition (autoencoders + quantum circuit + integration)
- âœ… Qiskit backend configuration for GPU-accelerated simulation (cuQuantum)
- âœ… Mixed precision training using `torch.cuda.amp`
- âœ… Dataset preprocessing and scaling on GPU. Custom dataloaders for GPU based data preprocessing and loading.
- âœ… Transitioned to PennyLane.
- âœ… Optimizing HQNN pipeline to have global parameters and making it more functional to automate it.
- âœ… Trained 30+ models in an epirical, systematic manner recording the logs(per epoch), metrics and parameters for different models on a subset of 1000 samples out of 22000 of total dataset for 20 epochs. (1 model is trained thrice with 3 different random seeds)
- âœ… Benchmark HQNN models with different params and create comparitive visualizations and interpret them.
- ğŸš§ Performance benchmarking against classical baselines
- ğŸš§ Create 1st draft of the results, providing solid proof of work and potential of the idea to approach institutes for computational resources and funding.

- ğŸ§  Imporvement opportunity identified:
-    - rn the autoencoders are optimized for 5 qubit circuits, in next iteration optimize them for 2,3,4 qubit systems also. RN complete this task with noisy simulations, and preprint then go for refinement.
     - rn standard scaler was used to scale the data, which might not be appropriate for option pricing.
  
Long term goals (next step)
- ğŸš§ Include noise simulation to simulate real quantum hardware
- ğŸš§ Compute on both data sets (Two major stock indices from 2 different markets NSE and Chicago stock exchange)
- ğŸš§ Implement on real quantum hardware, IBM runtime, Google, look for options
-  
---

## ğŸ’¬ Collaboration

Iâ€™m open to collaboration with:

- **Quantum ML researchers** interested in hybrid systems
- **Optimization experts** with experience in GPU acceleration or circuit transpilation or HPC.
- **Labs or academic mentors** working on interpretable quantum ML
- **Open-source contributors** who want to help shape the future of quantum-classical ML

Feel free to open a GitHub Issue or connect directly if you'd like to collaborate or follow along.

---
## ğŸ“ Current Project Structure (WIP) 

```bash
HQNN/
â”œâ”€â”€ ETEPipeline             # Latest progress on the HQNN pipeline (abstracted)
â”œâ”€â”€ Autoencoder             # Jupyter notebooks for deciding architecture of classical pre and post encoders
â”œâ”€â”€ environment.yml         # Conda environment file
â”œâ”€â”€ requirements.txt        # PIP dependencies (if not using conda)
â”œâ”€â”€ README.md               # Project overview
â”œâ”€â”€ .gitignore
```
## ğŸ“ Final Project Structure (WIP) (Expected)

Empty folder are deliberately kept empty to maintain confedentiality of the research

```bash
HQNN/
â”œâ”€â”€ notebooks/               # Jupyter notebooks for experiments, EDA, prototyping
â”‚   â””â”€â”€ pipeline_experiment.ipynb
â”‚   â””â”€â”€ qiskit_pipeline_v1.ipynb
â”‚
â”œâ”€â”€ src/                    # Source code (cleaned-up Python files)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model/              # Quantum + classical model components
â”‚   â”‚   â”œâ”€â”€ autoencoders.py
â”‚   â”‚   â”œâ”€â”€ hybrid_model.py
â”‚   â”‚   â””â”€â”€ quantum_circuit.py
â”‚   â”œâ”€â”€ training/           # Training and evaluation logic
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ eval.py
â”‚   â”œâ”€â”€ utils/              # Helpers for preprocessing, logging, plotting
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ visualizer.py
â”‚   â””â”€â”€ config.py           # Configs, hyperparameters, paths
â”‚
â”œâ”€â”€ data/                   # Processed input data (not tracked in git)
â”‚   â””â”€â”€ raw/            
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ checkpoints/            # Model checkpoints (not tracked in git)
â”‚
â”œâ”€â”€ outputs/                # Logs, plots, evaluation results
â”‚   â””â”€â”€ predictions/
â”‚
â”œâ”€â”€ tests/                  # Unit tests for components
â”‚   â””â”€â”€ test_model.py
â”‚
â”œâ”€â”€ environment.yml         # Conda environment file
â”œâ”€â”€ requirements.txt        # PIP dependencies (if not using conda)
â”œâ”€â”€ README.md               # Project overview
â”œâ”€â”€ .gitignore
â””â”€â”€ run_pipeline.py         # Not active as project in progress


